{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images are uploaded to S3 for storage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new S3 Bucket to store images\n",
    "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html\n",
    "\n",
    "### 1) Create a new S3 bucket \n",
    "Create and configure a new S3 bucket through the Amazon S3 console.  \n",
    "![Create and configure a new S3 bucket through the Amazon S3 console](img/S3_01.png)  \n",
    "\n",
    "\n",
    "### 2) Name the bucket\n",
    "Name the bucket according the DNS specifications.  \n",
    "Region chosen is Asia Pacific (Singapore) as this is where we are located.  \n",
    "![Name S3 bucket and choose region](img/S3_02.png)  \n",
    "\n",
    "\n",
    "### 3) Configure options  \n",
    "Check the \"Versioning - Keep all versions of an object in the same bucket\" box.  \n",
    "AWS Versioning Docs: https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html  \n",
    "Versioning allows us to keep multiple variants of an object in the same bucket. It helps to mitigate the risk of file incompatibility as it allows us to restore previous versions. It also helps us to keep track of our data. However, storing every single version can be expensive and AWS will charge for every additional Gigabyte used. As our dataset is still small, we will keep versioning on for now. Read this article for more information on versioning: \n",
    "https://medium.com/@pvinchon/amazon-s3-versioning-d6c57c513b04  \n",
    "![S3 Configure Options](img/S3_03.png) \n",
    "\n",
    "It can also be useful to enable \"server access logging\". This allows us to track requets for access to our bucket, therefore helping us understand S3 usage better. There is no extra charge for enabling server access logging on S3. However, it is recommended that logs be written to a different target bucket from the source. This is because additional logs are created for the logs written to the bucket and could make it difficult for us to find the relevant logs for our needs. Therefore, we may in future create a new target bucket t owrite those logs too. \n",
    "AWS logging Docs: https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html  \n",
    "\n",
    "\n",
    "### 4) Set permissions  \n",
    "Leave permission settings to default - \"Block all public access\"  \n",
    "These settings can be changed later if required.  \n",
    "![S3 Permissions](img/S3_04.png)  \n",
    "\n",
    "\n",
    "### 5) Review and create bucket \n",
    "Review settings and create bucket.  \n",
    "![Review S3 Settings](img/S3_05.png)  \n",
    "\n",
    "\n",
    "###  New bucket created! \n",
    "![New bucket created](img/S3_06.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the S3 Bucket \n",
    "\n",
    "To access the S3 bucket, we need an **access key** and **access secret**. This allows us to be able to make secure REST or HTTP Query protocal requests to AWS. There are a number of ways of accessing AWS resources. Read more about it [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html). \n",
    "\n",
    "To access our S3 bucket, we will create an **IAM user** to be able to [load data from AWS S3 into Google Colab](https://medium.com/python-in-plain-english/how-to-load-data-from-aws-s3-into-google-colab-7e76fbf534d2). \n",
    "\n",
    "A point to note: \n",
    "Long-term access keys (associated with IAM users and AWS account root users) never expire and remain valid until manually revoked. Therefore it may not be best practice due to security. Read more about [best practices](https://docs.aws.amazon.com/general/latest/gr/aws-access-keys-best-practices.html). \n",
    "\n",
    "Example: \n",
    "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Authorizing.IAM.S3CreatePolicy.html \n",
    "\n",
    "\n",
    "### 1) Create new IAM User\n",
    "- In the AWS console, click on your username and go to \"My Security Credentials\". \n",
    "- On the left menu bar,  click on \"Users\" then \"Add user\" \n",
    "\n",
    "![Add User](img/S3_07.png)\n",
    "\n",
    "\n",
    "### 2) Set user details\n",
    "- Give the user a name \n",
    "- Select programmatic access \n",
    "\n",
    "![Set User details](img/S3_08.png) \n",
    "\n",
    "\n",
    "### 3) Set permissions \n",
    "- Attach existing policies directly \n",
    "- Filter to S3 and choose \"AmazonS3FullAccess\" so that the Secret and Access keys generated will only allow access to the S3 we created and not any other AWS resources \n",
    "\n",
    "![Attach policy](img/S3_09.png) \n",
    "\n",
    "\n",
    "### 4) Add tags \n",
    "- Skipped for now \n",
    "\n",
    "![Add tags](img/S3_10.png)\n",
    "\n",
    "\n",
    "### 5) Review \n",
    "- check and review all settings then click create \n",
    "\n",
    "![Review user settings](img/S3_11.png) \n",
    "\n",
    "\n",
    "### 6) SUCCESS! \n",
    "- We have created our new IAM user!\n",
    "- An **access key** and **access secret** has been created :) \n",
    "- We will use this to access our S3 instance \n",
    "- Download the .csv file and save it somewhere \n",
    "\n",
    "![Access key and Secret](img/S3_12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access our S3 bucket via Python\n",
    "\n",
    "Let's test if it all works :) \n",
    "\n",
    "https://medium.com/python-in-plain-english/how-to-load-data-from-aws-s3-into-google-colab-7e76fbf534d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install AWS Python SDK (boto3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3==1.14.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you lose your keys, go to the AWS IAM console to manage access keys and generate a new set of keys \n",
    "- Set up Boto creditials to pull data from S3: \n",
    "\n",
    "```python\n",
    "BUCKET_NAME = 'xxxxxx' # replace with your bucket name\n",
    "\n",
    "# enter authentication credentials\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'ENTER YOUR ACCESS KEY',\n",
    "                    aws_secret_access_key = 'ENTER YOUR SECRET KEY')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "BUCKET_NAME = 'geoai-dtp-images'\n",
    "\n",
    "s3 = boto3.resource('s3', aws_access_key_id='ACCESS KEY',\n",
    "                   aws_secret_access_key='SECRET KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if it works \n",
    "To test that this all works, we will create a new file \"test.txt\", upload it to our S3 bucket, then download it locally with the following code.\n",
    "\n",
    "To upload the file into our bucket: \n",
    "```python\n",
    "s3.Bucket('BUCKET NAME').upload_file('NAME OF FILE TO UPLOAD', 'KEY OF FILE ON S3') \n",
    "```\n",
    "\n",
    "To download the file to our local directory: \n",
    "```python\n",
    "s3.Bucket('BUCKET NAME').download_file('KEY OF FILE ON S3', 'NAME OF DOWNLOADED FILE') \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code \n",
    "with open('test.txt', 'w') as test:\n",
    "    test.write(\"This is our S3 test file\")\n",
    "    \n",
    "KEY = 'test.txt' # replace with your object key\n",
    "\n",
    "s3.Bucket(BUCKET_NAME).upload_file(KEY, 'test.txt')\n",
    "s3.Bucket(BUCKET_NAME).download_file(KEY, 'downloaded_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if it has worked\n",
    "In the current directory, you should see two new files, \"test.txt\" and \"downoladed_test.txt\" \n",
    "![Check folder](img/S3_13.png)\n",
    "\n",
    "\n",
    "In our S3 Console, you should also see our \"test.txt\" file uploaded \n",
    "![Check S3 console](img/s3_14.png)\n",
    "\n",
    "We can now use this same code to access our S3 bucket via Google Colab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
